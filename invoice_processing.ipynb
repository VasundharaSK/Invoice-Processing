{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "invoice_processing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMr8d8cBkZdMMdtoQ6ml9D5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VasundharaSK/Invoice-Processing/blob/main/invoice_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFlHZDx_Z59J",
        "outputId": "3cf2e9f6-ff6d-4946-b5ab-0e359b07e21e"
      },
      "source": [
        "!git clone https://github.com/zzzDavid/ICDAR-2019-SROIE.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ICDAR-2019-SROIE'...\n",
            "remote: Enumerating objects: 2386, done.\u001b[K\n",
            "remote: Total 2386 (delta 0), reused 0 (delta 0), pack-reused 2386\u001b[K\n",
            "Receiving objects: 100% (2386/2386), 278.63 MiB | 26.40 MiB/s, done.\n",
            "Resolving deltas: 100% (211/211), done.\n",
            "Checking out files: 100% (1980/1980), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9Zv4VLPbYgy"
      },
      "source": [
        "## Creating folders for preprocessed dataset\n",
        "!mkdir boxes_and_transcripts images entities"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrBRzYt_blYe"
      },
      "source": [
        "## Script for preprocessing dataset\n",
        "import os\n",
        "import pandas\n",
        "import json\n",
        "import csv\n",
        "import shutil\n",
        "\n",
        "## Input dataset\n",
        "data_path = \"ICDAR-2019-SROIE/data/\"\n",
        "box_path = data_path + \"box/\"\n",
        "img_path = data_path + \"img/\"\n",
        "key_path = data_path + \"key/\"\n",
        "\n",
        "## Output dataset\n",
        "out_boxes_and_transcripts = \"/content/boxes_and_transcripts/\"\n",
        "out_images = \"/content/images/\"\n",
        "out_entities  = \"/content/entities/\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vla_1QOTb2pu",
        "outputId": "51cef04f-db46-4f94-dd6a-fa6c9c823079"
      },
      "source": [
        "train_samples_list =  []\n",
        "for file in os.listdir(data_path + \"box/\"):\n",
        "  \n",
        "  ## Reading csv\n",
        "  with open(box_path +file, \"r\") as fp:\n",
        "    reader = csv.reader(fp, delimiter=\",\")\n",
        "    ## arranging dataframe index ,coordinates x1_1,y1_1,x2_1,y2_1,x3_1,y3_1,x4_1,y4_1, transcript\n",
        "    rows = [[1] + x[:8] + [','.join(x[8:]).strip(',')] for x in reader] \n",
        "    df = pandas.DataFrame(rows)\n",
        "    ## including ner label dataframe index ,coordinates x1_1,y1_1,x2_1,y2_1,x3_1,y3_1,x4_1,y4_1, transcript , ner tag\n",
        "  df[10] = 'other'  \n",
        "  \n",
        "  ##saving file into new dataset folder\n",
        "  jpg = file.replace(\".csv\",\".jpg\")\n",
        "  entities = json.load(open(key_path+file.replace(\".csv\",\".json\")))\n",
        "  for key,value in sorted(entities.items()):\n",
        "    idx = df[df[9].str.contains('|'.join(map(str.strip, value.split(','))))].index\n",
        "    df.loc[idx, 10] = key\n",
        "\n",
        "  shutil.copy(img_path +jpg, out_images)\n",
        "  with open(out_entities + file.replace(\".csv\",\".txt\"),\"w\") as j:  \n",
        "    print(json.dumps(entities), file=j)\n",
        "  \n",
        "  df.to_csv(out_boxes_and_transcripts+file.replace(\".csv\",\".tsv\"),index=False,header=False, quotechar='',escapechar='\\\\',quoting=csv.QUOTE_NONE, )\n",
        "  train_samples_list.append(['receipt',file.replace('.csv','')])\n",
        "train_samples_list = pandas.DataFrame(train_samples_list)\n",
        "train_samples_list.to_csv(\"train_samples_list.csv\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/strings.py:2001: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
            "  return func(self, *args, **kwargs)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OluJrZDrdaMb"
      },
      "source": [
        "#add tag to tcsv file\n",
        "index ,x1_1,y1_1,x2_1,y2_1,x3_1,y3_1,x4_1,y4_1, transcript , ner tag\n",
        "1,72,25,326,25,326,64,72,64,TAN WOON YANN,other\n",
        "1,50,82,440,82,440,121,50,121,BOOK TA .K(TAMAN DAYA) SDN BND,address\n",
        "1,205,121,285,121,285,139,205,139,789417-W,other\n",
        "1,110,144,383,144,383,163,110,163,NO.53 55\\,57 & 59\\, JALAN SAGU 18,address\n",
        "1,192,169,299,169,299,187,192,187,TAMAN DAYA,address\n",
        "1,162,193,334,193,334,211,162,211,81100 JOHOR BAHRU,address\n",
        "1,217,216,275,216,275,233,217,233,JOHOR.,address\n",
        "1,50,342,279,342,279,359,50,359,DOCUMENT NO : TD01167104,other\n",
        "1,50,372,96,372,96,390,50,390,DATE:,other\n",
        "1,165,372,342,372,342,389,165,389,25/12/2018 8:13:39 PM,date "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "UkwXkam6cdVW",
        "outputId": "2b2edaf4-d958-484b-bbb2-68f6e9ebe703"
      },
      "source": [
        "## document_type, file_name\n",
        "train_samples_list.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>receipt</td>\n",
              "      <td>212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>receipt</td>\n",
              "      <td>300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>receipt</td>\n",
              "      <td>617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>receipt</td>\n",
              "      <td>253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>receipt</td>\n",
              "      <td>245</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0    1\n",
              "0  receipt  212\n",
              "1  receipt  300\n",
              "2  receipt  617\n",
              "3  receipt  253\n",
              "4  receipt  245"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu4aNFImdmYd"
      },
      "source": [
        "#Spliting dataset into train-test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_test = pandas.read_csv(\"train_samples_list.csv\",dtype=str)\n",
        "train, test= train_test_split(train_test,test_size=0.2,random_state = 42)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZQzYoA4mIy2"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GMj9JCN4Uo2",
        "outputId": "ec93b1bf-612f-4a1b-9aad-912d59eefca9"
      },
      "source": [
        "!git clone https://github.com/wenwenyu/PICK-pytorch.git #original repo\n",
        "## If you face any problem in training then clone my repo \n",
        "## which i have used while making this blog\n",
        "# !git clone https://github.com/dlmade/Pick.Pytorch.Sroie.git #my repo"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'PICK-pytorch'...\n",
            "remote: Enumerating objects: 293, done.\u001b[K\n",
            "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 293 (delta 19), reused 24 (delta 10), pack-reused 256\u001b[K\n",
            "Receiving objects: 100% (293/293), 9.99 MiB | 16.43 MiB/s, done.\n",
            "Resolving deltas: 100% (134/134), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC4N75ed49Dt"
      },
      "source": [
        "**Copy train data into PICK-pytorch data folder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ulyS4bIfIHi",
        "outputId": "18483110-4b1e-4de2-bd6e-bb6759c7e425"
      },
      "source": [
        "for index, row in train.iterrows():\n",
        "  shutil.copy(out_boxes_and_transcripts+str(row[2])+\".tsv\",'/content/PICK-pytorch/data/data_examples_root/boxes_and_transcripts/')\n",
        "  shutil.copy(out_images+str(row[2])+\".jpg\",'/content/PICK-pytorch/data/data_examples_root/images/')\n",
        "  shutil.copy(out_entities +str(row[2])+\".txt\", '/content/PICK-pytorch/data/data_examples_root/entities/')\n",
        "\n",
        "train.drop(['Unnamed: 0'], axis = 1,inplace = True)\n",
        "train.reset_index(inplace= True)\n",
        "train.drop(['index'], axis = 1,inplace = True)\n",
        "train.to_csv(\"/content/PICK-pytorch/data/data_examples_root/train_samples_list.csv\",header = False)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxecoY2zmgPY"
      },
      "source": [
        "Copy test data into PICK-pytorch data folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yem3DmSmi5U"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIuRK2bk5TmE"
      },
      "source": [
        "!mkdir '/content/PICK-pytorch/data/test_data_example/entities/'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUB25QI7mrSY",
        "outputId": "a4a6dd02-a32f-4ac9-dfb3-26d25d370d6e"
      },
      "source": [
        "for index, row in test.iterrows():\n",
        "  shutil.copy(out_boxes_and_transcripts+str(row[2])+\".tsv\",'/content/PICK-pytorch/data/test_data_example/boxes_and_transcripts/')\n",
        "  shutil.copy(out_images+str(row[2])+\".jpg\",'/content/PICK-pytorch/data/test_data_example/images/')\n",
        "  shutil.copy(out_entities +str(row[2])+\".txt\", '/content/PICK-pytorch/data/test_data_example/entities/')\n",
        "\n",
        "test.drop(['Unnamed: 0'], axis = 1,inplace = True)\n",
        "test.reset_index(inplace= True)\n",
        "test.drop(['index'], axis = 1,inplace = True)\n",
        "test.to_csv(\"/content/PICK-pytorch/data/test_data_example/test_samples_list.csv\",header = False)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgFm5eW4mvIp"
      },
      "source": [
        "## Removing data once it is copied into PICK-pytorch data folder\n",
        "!rm /content/boxes_and_transcripts/*.tsv\n",
        "!rm /content/images/*.jpg\n",
        "!rm /content/entities/*.txt"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz3nB-ihmzn7",
        "outputId": "7cab6052-2b59-42da-81a6-084d29a8035c"
      },
      "source": [
        "%cd PICK-pytorch/"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/PICK-pytorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDKtC69bm8Pr"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3_YpmKMWO28",
        "outputId": "e714885e-ad9c-4bea-8ba3-714d73aebb6d"
      },
      "source": [
        "%%writefile utils/entities_list.py\n",
        "# -*- coding: utf-8 -*-\n",
        "# @Author: Wenwen Yu\n",
        "# @Created Time: 7/8/2020 9:34 PM\n",
        "\n",
        "Entities_list = [\n",
        "    \"company\",\n",
        "    \"address\",\n",
        "    \"date\",\n",
        "    \"total\"\n",
        "]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting utils/entities_list.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "H5IFq09QnDxb",
        "outputId": "efe1238b-1a8c-4f16-9d57-027a67323724"
      },
      "source": [
        "## Installing requirements for running PICK-pytorch\n",
        "!pip install -r requirements.txt\n",
        "!pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchvision==0.6.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/ef/c32275c040f12f24adcd6302f0f3cd12cc432e408ce4ea521600e8fd989c/torchvision-0.6.1-cp37-cp37m-manylinux1_x86_64.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6MB 3.8MB/s \n",
            "\u001b[?25hCollecting tabulate==0.8.7\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/f4/770ae9385990f5a19a91431163d262182d3203662ea2b5739d0fcfc080f1/tabulate-0.8.7-py3-none-any.whl\n",
            "Collecting overrides==3.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/42/8d/caa729f809ecdf8e76fac3c1ff7d3f0b72c398c9dd8a6919927a30a873b3/overrides-3.0.0.tar.gz\n",
            "Collecting opencv_python==4.3.0.36\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/e6/c36487aacc7c37697634cf07b7c02684b292ea5cd5b6054a75f7e7d28d31/opencv_python-4.3.0.36-cp37-cp37m-manylinux2014_x86_64.whl (43.7MB)\n",
            "\u001b[K     |████████████████████████████████| 43.7MB 70kB/s \n",
            "\u001b[?25hCollecting numpy==1.16.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/d1/45be1144b03b6b1e24f9a924f23f66b4ad030d834ad31fb9e5581bd328af/numpy-1.16.4-cp37-cp37m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 208kB/s \n",
            "\u001b[?25hCollecting pandas==1.0.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/f3/683bf2547a3eaeec15b39cef86f61e921b3b187f250fcd2b5c5fb4386369/pandas-1.0.5-cp37-cp37m-manylinux1_x86_64.whl (10.1MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1MB 40.1MB/s \n",
            "\u001b[?25hCollecting allennlp==1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/49/bf0ec241496a82c9dd2f0b6ff6f8156b6b2b72b849df8c00a4f2bcf61485/allennlp-1.0.0-py3-none-any.whl (473kB)\n",
            "\u001b[K     |████████████████████████████████| 481kB 39.0MB/s \n",
            "\u001b[?25hCollecting torchtext==0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/17/e7c588245aece7aa93f360894179374830daf60d7ed0bbb59332de3b3b61/torchtext-0.6.0-py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 208kB/s \n",
            "\u001b[?25hCollecting tqdm==4.47.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/62/7663894f67ac5a41a0d8812d78d9d2a9404124051885af9d77dc526fb399/tqdm-4.47.0-py2.py3-none-any.whl (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.2MB/s \n",
            "\u001b[?25hCollecting torch==1.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/cf/007b6de316c9f3d4cb315a60c308342cc299e464167f5ebc369e93b5e23a/torch-1.5.1-cp37-cp37m-manylinux1_x86_64.whl (753.2MB)\n",
            "\u001b[K     |████████████████████████████████| 753.2MB 20kB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.6.1->-r requirements.txt (line 1)) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandas==1.0.5->-r requirements.txt (line 6)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.0.5->-r requirements.txt (line 6)) (2018.9)\n",
            "Collecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/40/6f16e5ac994b16fa71c24310f97174ce07d3a97b433275589265c6b94d2b/jsonnet-0.17.0.tar.gz (259kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 35.1MB/s \n",
            "\u001b[?25hCollecting tensorboardX>=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/84/46421bd3e0e89a92682b1a38b40efc22dafb6d8e3d947e4ceefd4a5fabc7/tensorboardX-2.2-py2.py3-none-any.whl (120kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 41.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp==1.0.0->-r requirements.txt (line 7)) (0.22.2.post1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp==1.0.0->-r requirements.txt (line 7)) (2.10.0)\n",
            "Requirement already satisfied: spacy<2.3,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp==1.0.0->-r requirements.txt (line 7)) (2.2.4)\n",
            "Requirement already satisfied: filelock<3.1,>=3.0 in /usr/local/lib/python3.7/dist-packages (from allennlp==1.0.0->-r requirements.txt (line 7)) (3.0.12)\n",
            "Collecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/1a/f2db026d4d682303793559f1c2bb425ba3ec0d6fd7ac63397790443f2461/jsonpickle-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp==1.0.0->-r requirements.txt (line 7)) (3.6.4)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/8f/42959300c543b4d34bc9f9b54954471a33384c181084ed84f070763d7f37/boto3-1.17.62-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 42.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp==1.0.0->-r requirements.txt (line 7)) (3.2.5)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp==1.0.0->-r requirements.txt (line 7)) (2.23.0)\n",
            "Collecting transformers<2.12,>=2.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 36.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp==1.0.0->-r requirements.txt (line 7)) (1.4.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 36.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0->-r requirements.txt (line 8)) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.5.1->-r requirements.txt (line 10)) (0.16.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX>=1.2->allennlp==1.0.0->-r requirements.txt (line 7)) (3.12.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->allennlp==1.0.0->-r requirements.txt (line 7)) (1.0.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<2.3,>=2.1.0->allennlp==1.0.0->-r requirements.txt (line 7)) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.3,>=2.1.0->allennlp==1.0.0->-r requirements.txt (line 7)) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.3,>=2.1.0->allennlp==1.0.0->-r requirements.txt (line 7)) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.3,>=2.1.0->allennlp==1.0.0->-r requirements.txt (line 7)) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<2.3,>=2.1.0->allennlp==1.0.0->-r requirements.txt (line 7)) (56.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.3,>=2.1.0->allennlp==1.0.0->-r requirements.txt (line 7)) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<2.3,>=2.1.0->allennlp==1.0.0->-r requirements.txt (line 7)) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.3,>=2.1.0->allennlp==1.0.0->-r requirements.txt (line 7)) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.3,>=2.1.0->allennlp==1.0.0->-r requirements.txt (line 7)) (0.8.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.3,>=2.1.0->allennlp==1.0.0->-r requirements.txt (line 7)) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from jsonpickle->allennlp==1.0.0->-r requirements.txt (line 7)) (3.10.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==1.0.0->-r requirements.txt (line 7)) (20.3.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==1.0.0->-r requirements.txt (line 7)) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==1.0.0->-r requirements.txt (line 7)) (8.7.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==1.0.0->-r requirements.txt (line 7)) (1.10.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==1.0.0->-r requirements.txt (line 7)) (1.4.0)\n",
            "Collecting s3transfer<0.5.0,>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.4MB/s \n",
            "\u001b[?25hCollecting botocore<1.21.0,>=1.20.62\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/60/ba830f93176fdc23166043298173ee2aecd5cf150f1ede51d6506f021deb/botocore-1.20.62-py2.py3-none-any.whl (7.5MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5MB 38.2MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp==1.0.0->-r requirements.txt (line 7)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp==1.0.0->-r requirements.txt (line 7)) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp==1.0.0->-r requirements.txt (line 7)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp==1.0.0->-r requirements.txt (line 7)) (2.10)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/59/bb06dd5ca53547d523422d32735585493e0103c992a52a97ba3aa3be33bf/tokenizers-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 32.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<2.12,>=2.9->allennlp==1.0.0->-r requirements.txt (line 7)) (20.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 38.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<2.12,>=2.9->allennlp==1.0.0->-r requirements.txt (line 7)) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle->allennlp==1.0.0->-r requirements.txt (line 7)) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle->allennlp==1.0.0->-r requirements.txt (line 7)) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<2.12,>=2.9->allennlp==1.0.0->-r requirements.txt (line 7)) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<2.12,>=2.9->allennlp==1.0.0->-r requirements.txt (line 7)) (7.1.2)\n",
            "Building wheels for collected packages: overrides, jsonnet\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.0.0-cp37-none-any.whl size=5669 sha256=fd912a1f3f247c66d5c957199f827c77ac83ad94ce8ce168de5cb46cb6af5f4b\n",
            "  Stored in directory: /root/.cache/pip/wheels/6f/1b/ec/6c71a1eb823df7f850d956b2d8c50a6d49c191e1063d73b9be\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.17.0-cp37-cp37m-linux_x86_64.whl size=3388778 sha256=e7936c578cf3e10b43a1fa3857b7dabf4f8479d631a4b82cf9a2ec535dec5d53\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/7a/37/7dbcc30a6b4efd17b91ad1f0128b7bbf84813bd4e1cfb8c1e3\n",
            "Successfully built overrides jsonnet\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pyarrow 3.0.0 has requirement numpy>=1.16.6, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 1.0.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: astropy 4.2.1 has requirement numpy>=1.17, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: botocore 1.20.62 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, torch, torchvision, tabulate, overrides, opencv-python, pandas, tqdm, jsonnet, tensorboardX, jsonpickle, jmespath, botocore, s3transfer, boto3, tokenizers, sacremoses, sentencepiece, transformers, allennlp, torchtext\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "  Found existing installation: torchvision 0.9.1+cu101\n",
            "    Uninstalling torchvision-0.9.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.9.1+cu101\n",
            "  Found existing installation: tabulate 0.8.9\n",
            "    Uninstalling tabulate-0.8.9:\n",
            "      Successfully uninstalled tabulate-0.8.9\n",
            "  Found existing installation: opencv-python 4.1.2.30\n",
            "    Uninstalling opencv-python-4.1.2.30:\n",
            "      Successfully uninstalled opencv-python-4.1.2.30\n",
            "  Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Found existing installation: torchtext 0.9.1\n",
            "    Uninstalling torchtext-0.9.1:\n",
            "      Successfully uninstalled torchtext-0.9.1\n",
            "Successfully installed allennlp-1.0.0 boto3-1.17.62 botocore-1.20.62 jmespath-0.10.0 jsonnet-0.17.0 jsonpickle-2.0.0 numpy-1.16.4 opencv-python-4.3.0.36 overrides-3.0.0 pandas-1.0.5 s3transfer-0.4.2 sacremoses-0.0.45 sentencepiece-0.1.95 tabulate-0.8.7 tensorboardX-2.2 tokenizers-0.7.0 torch-1.5.1 torchtext-0.6.0 torchvision-0.6.1 tqdm-4.47.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pandas"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp37-cp37m-linux_x86_64.whl (704.4MB)\n",
            "\u001b[K     |████████████████████████████████| 704.4MB 26kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.1%2Bcu101-cp37-cp37m-linux_x86_64.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 27.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.5.1+cu101) (1.16.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.6.1+cu101) (7.1.2)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.5.1\n",
            "    Uninstalling torch-1.5.1:\n",
            "      Successfully uninstalled torch-1.5.1\n",
            "  Found existing installation: torchvision 0.6.1\n",
            "    Uninstalling torchvision-0.6.1:\n",
            "      Successfully uninstalled torchvision-0.6.1\n",
            "Successfully installed torch-1.5.1+cu101 torchvision-0.6.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5OLWWTOryh4",
        "outputId": "552f97e7-6aa8-4c6f-d764-8a6756649be2"
      },
      "source": [
        "%%writefile config.json\n",
        "\n",
        "{\n",
        "    \"name\": \"PICK_Default\",\n",
        "    \"run_id\":\"test\",\n",
        "\n",
        "    \"local_world_size\":4,\n",
        "    \"local_rank\":-1,\n",
        "    \"distributed\":-1,\n",
        "\n",
        "    \"model_arch\": {\n",
        "        \"type\": \"PICKModel\",\n",
        "        \"args\": {\n",
        "            \"embedding_kwargs\":{\n",
        "                \"num_embeddings\": -1,\n",
        "                \"embedding_dim\": 512\n",
        "            },\n",
        "            \"encoder_kwargs\":{\n",
        "                \"char_embedding_dim\":-1,\n",
        "                \"out_dim\": 512,\n",
        "                \"nheaders\": 4,\n",
        "                \"nlayers\": 3,\n",
        "                \"feedforward_dim\": 1024,\n",
        "                \"dropout\": 0.1,\n",
        "                \"image_encoder\": \"resnet50\",\n",
        "                \"roi_pooling_mode\": \"roi_align\",\n",
        "                \"roi_pooling_size\": [7,7]\n",
        "            },\n",
        "            \"graph_kwargs\":{\n",
        "                \"in_dim\":-1,\n",
        "                \"out_dim\":-1,\n",
        "                \"eta\": 1,\n",
        "                \"gamma\": 1,\n",
        "                \"learning_dim\": 128,\n",
        "                \"num_layers\": 2\n",
        "            },\n",
        "            \"decoder_kwargs\":{\n",
        "                \"bilstm_kwargs\":{\n",
        "                    \"input_size\": -1,\n",
        "                     \"hidden_size\": 512,\n",
        "                     \"num_layers\": 2,\n",
        "                     \"dropout\": 0.1,\n",
        "                     \"bidirectional\": true,\n",
        "                     \"batch_first\": true\n",
        "\n",
        "                },\n",
        "                \"mlp_kwargs\":{\n",
        "                     \"in_dim\": -1,\n",
        "                     \"out_dim\": -1,\n",
        "                    \"dropout\": 0.1\n",
        "                },\n",
        "                \"crf_kwargs\":{\n",
        "                    \"num_tags\":-1\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"train_dataset\": {\n",
        "        \"type\": \"PICKDataset\",\n",
        "        \"args\": {\n",
        "            \"files_name\":\"/content/PICK-pytorch/data/data_examples_root/train_samples_list.csv\",\n",
        "            \"boxes_and_transcripts_folder\":\"/content/PICK-pytorch/data/data_examples_root/boxes_and_transcripts\",\n",
        "            \"images_folder\":\"/content/PICK-pytorch/data/data_examples_root/images\",\n",
        "            \"entities_folder\":\"/content/PICK-pytorch/data/data_examples_root/entities\",\n",
        "            \"iob_tagging_type\":\"box_and_within_box_level\",\n",
        "            \"resized_image_size\": [480, 960],\n",
        "            \"ignore_error\": false\n",
        "        }\n",
        "    },\n",
        "    \"validation_dataset\": {\n",
        "        \"type\": \"PICKDataset\",\n",
        "        \"args\": {\n",
        "            \"files_name\":\"/content/PICK-pytorch/data/test_data_example/test_samples_list.csv\",\n",
        "            \"boxes_and_transcripts_folder\":\"/content/PICK-pytorch/data/test_data_example/boxes_and_transcripts\",\n",
        "            \"images_folder\":\"/content/PICK-pytorch/data/test_data_example/images\",\n",
        "            \"entities_folder\":\"/content/PICK-pytorch/data/test_data_example/entities\",\n",
        "            \"iob_tagging_type\":\"box_and_within_box_level\",\n",
        "            \"resized_image_size\": [480, 960],\n",
        "            \"ignore_error\": false\n",
        "        }\n",
        "    },\n",
        "    \"train_data_loader\": {\n",
        "        \"type\": \"DataLoader\",\n",
        "        \"args\":{\n",
        "            \"batch_size\": 4,\n",
        "            \"shuffle\": true,\n",
        "            \"drop_last\": true,\n",
        "            \"num_workers\": 8,\n",
        "            \"pin_memory\":true\n",
        "        }\n",
        "    },\n",
        "    \"val_data_loader\": {\n",
        "          \"type\": \"DataLoader\",\n",
        "          \"args\":{\n",
        "              \"batch_size\": 4,\n",
        "              \"shuffle\": false,\n",
        "              \"drop_last\": false,\n",
        "              \"num_workers\": 8,\n",
        "              \"pin_memory\":true\n",
        "          }\n",
        "      },\n",
        "\n",
        "    \"optimizer\": {\n",
        "          \"type\": \"Adam\",\n",
        "          \"args\":{\n",
        "              \"lr\": 0.0001,\n",
        "              \"weight_decay\": 0,\n",
        "              \"amsgrad\": true\n",
        "          }\n",
        "    },\n",
        "    \"lr_scheduler\": {\n",
        "        \"type\": \"StepLR\",\n",
        "        \"args\": {\n",
        "            \"step_size\": 30,\n",
        "            \"gamma\": 0.1\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"trainer\": {\n",
        "        \"epochs\": 100,\n",
        "        \"gl_loss_lambda\": 0.01,\n",
        "        \"log_step_interval\": 10,\n",
        "        \"val_step_interval\": 50,\n",
        "\n",
        "        \"save_dir\": \"saved/\",\n",
        "        \"save_period\": 20,\n",
        "        \"log_verbosity\": 2,\n",
        "\n",
        "        \"monitor\": \"max overall-mEF\",\n",
        "        \"monitor_open\": true,\n",
        "        \"early_stop\": 40,\n",
        "\n",
        "        \"anomaly_detection\": false,\n",
        "        \"tensorboard\": false,\n",
        "\n",
        "        \"sync_batch_norm\":true\n",
        "    }\n",
        "}"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting config.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApNPD4ghnnQB"
      },
      "source": [
        "Training\n",
        "\n",
        "Train atleast 100 epoch for better results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gB5bLkaNn2t3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18e7d90d-ae6d-48cf-a664-a02fee148672"
      },
      "source": [
        "#!/bin/bash\n",
        "!python -m torch.distributed.launch --nnode=1 --node_rank=0 --nproc_per_node=1 \\\n",
        "   train.py -c config.json -d 0 --local_world_size 1\n",
        "  # --resume /content/PICK-pytorch/saved/models/PICK_Default/test_0917_074722/model_best.pth\n",
        "   ##uncomment for resume training"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-01 12:40:32.033012: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "[2021-05-01 12:40:36,061 - train - INFO] - Distributed GPU training model start...\n",
            "[2021-05-01 12:40:36,062 - train - INFO] - [Process 281] Initializing process group with: {'MASTER_ADDR': '127.0.0.1', 'MASTER_PORT': '29500', 'RANK': '0', 'WORLD_SIZE': '1'}\n",
            "[2021-05-01 12:40:36,063 - train - INFO] - [Process 281] world_size = 1, rank = 0, backend=nccl\n",
            "[2021-05-01 12:40:36,077 - train - INFO] - Dataloader instances created. Train datasets: 500 samples Validation datasets: 126 samples.\n",
            "[2021-05-01 12:40:37,089 - train - INFO] - Model created, trainable parameters: 68567386.\n",
            "[2021-05-01 12:40:37,091 - train - INFO] - Optimizer and lr_scheduler created.\n",
            "[2021-05-01 12:40:37,091 - train - INFO] - Max_epochs: 100 Log_per_step: 10 Validation_per_step: 50.\n",
            "[2021-05-01 12:40:37,092 - train - INFO] - Training start...\n",
            "[2021-05-01 12:40:37,095 - trainer - INFO] - [Process 281] world_size = 1, rank = 0, n_gpu/process = 1, device_ids = [0]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/distributed/distributed_c10d.py:102: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
            "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
            "[2021-05-01 12:41:30,026 - trainer - INFO] - Train Epoch:[1/100] Step:[10/250] Total Loss: 353.889465 GL_Loss: 6.386525 CRF_Loss: 347.502930\n",
            "[2021-05-01 12:42:08,046 - trainer - INFO] - Train Epoch:[1/100] Step:[20/250] Total Loss: 423.823151 GL_Loss: 0.718475 CRF_Loss: 423.104675\n",
            "[2021-05-01 12:42:45,912 - trainer - INFO] - Train Epoch:[1/100] Step:[30/250] Total Loss: 538.236328 GL_Loss: 0.167835 CRF_Loss: 538.068481\n",
            "[2021-05-01 12:43:25,019 - trainer - INFO] - Train Epoch:[1/100] Step:[40/250] Total Loss: 479.562714 GL_Loss: 0.291466 CRF_Loss: 479.271240\n",
            "[2021-05-01 12:44:04,373 - trainer - INFO] - Train Epoch:[1/100] Step:[50/250] Total Loss: 541.503052 GL_Loss: 0.163965 CRF_Loss: 541.339111\n",
            "[2021-05-01 12:45:08,433 - trainer - INFO] - [Step Validation] Epoch:[1/100] Step:[50/250]  \n",
            "+---------+-------+-------+-------+-------+\n",
            "| name    |   mEP |   mER |   mEF |   mEA |\n",
            "+=========+=======+=======+=======+=======+\n",
            "| date    |     0 |     0 |     0 |     0 |\n",
            "+---------+-------+-------+-------+-------+\n",
            "| address |     0 |     0 |     0 |     0 |\n",
            "+---------+-------+-------+-------+-------+\n",
            "| total   |     0 |     0 |     0 |     0 |\n",
            "+---------+-------+-------+-------+-------+\n",
            "| company |     0 |     0 |     0 |     0 |\n",
            "+---------+-------+-------+-------+-------+\n",
            "| overall |     0 |     0 |     0 |     0 |\n",
            "+---------+-------+-------+-------+-------+\n",
            "[2021-05-01 12:45:12,063 - trainer - INFO] - Saving current best: model_best.pth ...\n",
            "[2021-05-01 12:45:50,459 - trainer - INFO] - Train Epoch:[1/100] Step:[60/250] Total Loss: 910.060242 GL_Loss: 0.138731 CRF_Loss: 909.921509\n",
            "[2021-05-01 12:46:29,528 - trainer - INFO] - Train Epoch:[1/100] Step:[70/250] Total Loss: 376.439697 GL_Loss: 0.286608 CRF_Loss: 376.153076\n",
            "[2021-05-01 12:47:07,368 - trainer - INFO] - Train Epoch:[1/100] Step:[80/250] Total Loss: 423.114227 GL_Loss: 0.206750 CRF_Loss: 422.907471\n",
            "[2021-05-01 12:47:45,623 - trainer - INFO] - Train Epoch:[1/100] Step:[90/250] Total Loss: 386.337341 GL_Loss: 0.160819 CRF_Loss: 386.176514\n",
            "[2021-05-01 12:48:22,768 - trainer - INFO] - Train Epoch:[1/100] Step:[100/250] Total Loss: 366.535950 GL_Loss: 0.173577 CRF_Loss: 366.362366\n",
            "[2021-05-01 12:49:26,649 - trainer - INFO] - [Step Validation] Epoch:[1/100] Step:[100/250]  \n",
            "+---------+-------+-------+-------+-------+\n",
            "| name    |   mEP |   mER |   mEF |   mEA |\n",
            "+=========+=======+=======+=======+=======+\n",
            "| date    |     0 |     0 |     0 |     0 |\n",
            "+---------+-------+-------+-------+-------+\n",
            "| address |     0 |     0 |     0 |     0 |\n",
            "+---------+-------+-------+-------+-------+\n",
            "| total   |     0 |     0 |     0 |     0 |\n",
            "+---------+-------+-------+-------+-------+\n",
            "| company |     0 |     0 |     0 |     0 |\n",
            "+---------+-------+-------+-------+-------+\n",
            "| overall |     0 |     0 |     0 |     0 |\n",
            "+---------+-------+-------+-------+-------+\n",
            "[2021-05-01 12:49:30,478 - trainer - INFO] - Saving current best: model_best.pth ...\n",
            "[2021-05-01 12:50:09,113 - trainer - INFO] - Train Epoch:[1/100] Step:[110/250] Total Loss: 391.876190 GL_Loss: 0.179606 CRF_Loss: 391.696594\n",
            "[2021-05-01 12:50:47,292 - trainer - INFO] - Train Epoch:[1/100] Step:[120/250] Total Loss: 421.412781 GL_Loss: 0.435547 CRF_Loss: 420.977234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdibqAvmtHwy"
      },
      "source": [
        "Testing \n",
        "\n",
        "create folder for testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR3S67H4tLxc"
      },
      "source": [
        "##creating testing folders\n",
        "!mkdir /content/test_img /content/test_boxes_and_transcripts\n",
        "\n",
        "## copy one file from test sample\n",
        "import os\n",
        "import shutil\n",
        "data_path = \"data/test_data_example/boxes_and_transcripts/\"\n",
        "image_path = \"data/test_data_example/images/\"\n",
        "\n",
        "out_img_path = \"/content/test_img/\"\n",
        "out_box_path = \"/content/test_boxes_and_transcripts/\"\n",
        "\n",
        "for file in os.listdir(data_path)[:10]:\n",
        "  shutil.copy(data_path+file,out_box_path)\n",
        "  shutil.copy(image_path+file.replace(\".tsv\",\".jpg\"),out_img_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iikz012-t3Vw"
      },
      "source": [
        "## change model_best.pth path\n",
        "!python test.py --checkpoint saved/models/PICK_Default/test_1003_053713/model_best.pth \\\n",
        "                --boxes_transcripts {out_box_path} \\\n",
        "                --images_path {out_img_path} --output_folder /content/output/ \\\n",
        "                --gpu 0 --batch_size 2"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}